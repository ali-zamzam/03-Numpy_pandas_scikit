"""Classification"""

"""The most significant difference between regression vs classification is that 
- regression helps predict a continuous quantity (example: the price) 
- classification predicts discrete class labels.(example: Spam no spam)
- These values can be numeric or literal but in both cases the target variable takes a finite number of values.
The different values taken by the target variable are what are called classes."""


"""Scikit-learn offers many classification models that can be grouped into two families:

- Linear models like LogisticRegression.
- Non-linear models like KNeighborsClassifier.
- Using these models is the same for all scikit-learn models:

**models instantiation**.
- Model training: model.fit(X_train, y_train).
- Prediction: model.predict(X_test).
- The prediction on the test set allows us to evaluate the performance of the model thanks to adapted metrics.


**The metrics we have seen are used for binary classification and are calculated using 4 values:**

- True Positives: Prediction = + | Reality = +
- True Negatives: Prediction = - | Reality = -
- False Positives: Prediction = + | Reality = -
- False Negatives: Prediction = - | Reality = +
All these values can be calculated using the confusion matrix generated by the confusion_matrix function of the sklearn.metrics sub-module or by the pd.crosstab function.


Thanks to these values, we can calculate metrics like:

- Accuracy: The proportion of correctly classified observations.
- Precision: The proportion of true positives among all positive predictions of the model.
- Recall: The proportion of truly positive observations that have been correctly classified as positive by the model.

All these metrics can be obtained using the **classification_report** function of the sklearn.metrics sub-module.

- The F1-Score quantifies the balance between these metrics, which gives us a reliable criterion to choose the most 
suitable model for our problem.
- The F1-Score is a kind of average between precision and recall.
- The F1-Score adapts very well to classification problems with balanced or unbalanced classes.
- For most classification problems, the model with the highest F1-Score will be considered as the model 
whose recall and precision performance are the most balanced,and is therefore preferable to the others."""
# ------------------------------------------------------------------------------------------------
"""example **voting records**"""
"""The explanatory variables(features) will therefore be the votes on different subjects
and the target(label) variable will be the "democratic" or "republican" political party."""

# To solve this problem we will use:
"""
- A non-linear classification model: K-Nearest Neighbors.
- A linear classification model: Logistic Regression."""


# (%matplotlib inline)    # enable the inline plotting, where the plots/graphs will be displayed 
# just below the cell where your plotting commands are written.
#  It provides interactivity with the backend in the frontends like the jupyter notebook.


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, classification_report,
                             confusion_matrix, f1_score, precision_score,
                             recall_score)
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

import numpy as np
import pandas as pd

%matplotlib inline     

votes = pd.read_csv("data/house-votes-84.csv")

votes.head()
# 	Class Name	handicapped-infants	water-project-cost-sharing	....	export-administration-act-south-africa
# 0	republican	n	y	n	y	y	y	n	n	n	y	?	y	....   	y
# 1	republican	n	y	n	y	y	y	n	n	n	n	n	y	....  	?
# 2	democrat	?	y	y	?	y	y	n	n	n	n	y	n	....  	n
# 3	democrat	n	y	y	n	?	y	n	n	n	n	y	n	....  	y
# 4	democrat	y	y	y	n	y	y	n	n	n	n	y	?	....    y

# display the shape of dataframe
votes.shape

# display the number of rows in the dataframe
votes.shape[0]
# 453 rows
votes.head()

votes = votes.replace("?", np.nan)

# replace (y and n) with (1 and 0)
votes = votes.replace(("y","n"), (1,0))
# 	Class Name	handicapped-infants	water-project-cost-sharing	....	export-administration-act-south-africa
# 0	republican	0	1	0	1	1	1	0	0	0	1	?	1	....   	1
# 1	republican	0	1	0	1	1	1	0	0	0	0	0	1	....  	?
# 2	democrat	?	1	y	?	1	1	0	0	0	0	1	0	....  	0
# 3	democrat	0	1	y	0	?	1	0	0	0	0	1	0	....  	1
# 4	democrat	1	1	y	0	1	1	0	0	0	0	1	?	....    1


df= votes.dropna(axis = 0, how = 'any')

df.isnull().sum()
df.dtypes

df.head()

X = df.drop(["Class Name"], axis = 1)

y = df["Class Name"]



### we must import train_test_split from sklearn.model_selection ##
"""To eliminate the randomness of the train_test_split function we use **randomstate**"""
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)

"""we will use K-Nearest Neighbors algorithm"""
# Suppose K = 5.
# For an observation that we want to classify, we will look at the 5 points of the training set closest to our observation.
# If among the 5 neighbors, the majority is "Democrat", then the observation will be classified as "Democrat".

## we must import KNeighborsClassifier from sklearn.neighbors ##
knn = KNeighborsClassifier(n_neighbors = 6)

knn.fit(X_train, y_train)

y_pred_test_knn = knn.predict(X_test)

# display the first 10 predictions.
print(y_pred_test_knn[:10])
# ['democrat' 'republican' 'republican' 'democrat' 'democrat' 'republican'
#  'republican' 'republican' 'democrat' 'democrat']
# --------------------------------------------------------------------------
"""Linear Classification: Logistic Regression"""
"""
-The logistic regression model is closely related to the linear regression model
- but **Logistic regression** is used for classification (predicting classes)."""

## we must import LogisticRegression from sklearn.linear_model ##
logreg = LogisticRegression()

logreg.fit(X_train, y_train)
y_pred_test_logreg = logreg.predict(X_test)

print(y_pred_test_logreg[:10])
['democrat' 'republican' 'republican' 'democrat' 'democrat' 'republican'
 'republican' 'republican' 'democrat' 'democrat']

# ------------------------------------------------------------------------------------------------
"""Evaluate the performance of a classification model"""

# There are different metrics to evaluate the performance of classification models such as:
"""
- **Accuracy**.
- **Precision and recall**.
- **The F1 score, or F1-score**.
"""
# Each metric evaluates model performance with a different approach.

#  In order to explain these notions, we are going to introduce 4 very important terms.
#  Arbitrarily, we will choose that the class 'republican' will be the positive class (1) and
#  'democrat' will be the negative class (0).
"""
Thus, we will call:
- True positive (VP) an observation classified as positive ('republican') by the model and which is actually positive ('republican').
- False positive (FP) an observation classified as positive ('republican') by the model but which was actually negative ('democrat').
- True negative (VN) an observation classified as negative ('democrat') by the model and which is actually negative ('democrat').
- False negative (FN) an observation classified as negative ('democrat') by the model but which was actually positive ('republican')."""

# ------------------------------------------------------------------------------------------------
"""Accuracy"""

# Accuracy is the most commonly used metric to evaluate a model.
# It simply corresponds to the **rate** of **correct predictions** made by the model.

# We assume that we have n observations.
# We note VP the number of True Positives and VN the number of True Negatives.
"""The accuracy is then given by:
accuracy=(VP+VN)/n
"""


"""precision"""
# precision is a metric that answers the question: Of all the positive predictions in the model, 
# how many are true positives?
# If we note FP the number of False Positives of the model, then:
"""the precision is given by:
precision=VP/(VP+FP)"""
# A high precision score tells us that the model is not blindly classifying all observations as positive.


"""Recall"""
# Recall is a metric that quantifies the proportion of truly positive observations that have been correctly 
# classified as positive by the model.

# If we note FN the number of False Negatives, then 
"""the recall is given by:
callback=VP/(VP+FN)
"""
# A high recall score informs us that the model is able to detect really positive observations well.


"""confusion matrix"""
# The confusion matrix counts for a dataset the values ​​of VP, VN, FP and FN, which allows us 
# to calculate the three previous metrics:
""" ConfusionMatrix=[[VN FP]
                     [FN VP]]
                     """

"""
The confusion_matrix function of the sklearn.metrics sub-module is used to generate the confusion matrix 
from the predictions of a model:"""

# confusion_matrix(y_true, y_pred)
# y_true contains the true values ​​of y.
# y_pred contains the values ​​predicted by the model.

# --------------------------------------------------------------------------------------------------------
## we must import confusion_matrix from sklearn.metrics ## 

confusion = confusion_matrix(y_test, y_pred_test_knn)
print("Confusionmatrix:\n",  confusion)
# Confusionmatrix:
#  [[21  1]
#  [ 1 24]]


# How many false positives have occurred? 
# The True positive corresponds to 'republican'.

print("we have:\n",  confusion[0,1], "Faux positive")
# confusion[0,1] using numpy to search the index
# we have:
#  1 Faux positive


"""calculate the accuracy, precision and recall of the knn model on the test set"""
(VN, FP), (FN, VP) = confusion_matrix(y_test, y_pred_test_knn)
n = len(y_test)

print("Accuracy:", (VP + VN) / n)
# Accuracy: 0.9574468085106383

print("Precision:", VP / (VP + FP))
# Precision: 0.96

print("Recall:", VP / (VP + FN))
# Recall: 0.96

# -----------------------------------------------------------------------------------
"""The display of the confusion matrix can also be done with the pd.crosstab function""" 

pd.crosstab(y_test, y_pred_test_knn, rownames=['Reality'], colnames=['Prediction'])

# Prediction	democrat	republican
# Reality		
# democrat	        21	    1
# republican	    1	    24


"""If you think you can't remember the formulas for the accuracy, precision, and recall metrics,
don't worry! The sklearn.metrics submodule contains the functions to calculate them quickly:"""

## we must import accuracy_score, precision_score, recall_score from sklearn.metrics ##

pd.crosstab(y_test, y_pred_test_logreg, rownames=['Reality'], colnames=['Prediction'])

print("LogReg Accuracy:", accuracy_score(y_test, y_pred_test_logreg))
# LogReg Accuracy: 0.9787234042553191


"""it will be necessary to inform the argument pos_label = 'republican' in order to specify that 
the class 'republican' is the positive class."""

print("LogReg Precision:", precision_score(y_test, y_pred_test_logreg, pos_label = 'republican'))
# LogReg Precision: 0.9615384615384616

print("LogReg Rappel:", recall_score(y_test, y_pred_test_logreg, pos_label = 'republican'))
# LogReg Rappel: 1.0

# ----------------------------------------------------------------------------------------
"""classification_report"""
"""The classification_report function of the sklearn.metrics submodule displays all these metrics for each class."""

## we must import classification_report from sklearn.metrics ## 

print(classification_report(y_test, y_pred_test_logreg))
#                 precision    recall  f1-score   support

#     democrat       1.00      0.95      0.98        22
#   republican       0.96      1.00      0.98        25

#     accuracy                           0.98        47
#    macro avg       0.98      0.98      0.98        47
# weighted avg       0.98      0.98      0.98        47



print(classification_report(y_test, y_pred_test_knn))

#                 precision    recall  f1-score   support

#     democrat       0.95      0.95      0.95        22
#   republican       0.96      0.96      0.96        25

#     accuracy                           0.96        47
#    macro avg       0.96      0.96      0.96        47
# weighted avg       0.96      0.96      0.96        47
# ------------------------------------------------------------------------------------------
"""F1-Score"""
"""The classification report is a bit more comprehensive than what we have done so far.
It contains an additional metric: the F1-Score 

- The F1-Score is a kind of average between precision and recall.
- The F1-Score adapts very well to classification problems with balanced or unbalanced classes.
- For most classification problems, the model with the highest F1-Score will be considered as the model 
whose recall and precision performance are the most balanced,and is therefore preferable to the others."""

## we must import f1_score  from sklearn.metrics ##

print("F1 KNN:", f1_score(y_test, y_pred_test_knn, pos_label = 'republican'))
# F1 KNN: 0.96

print("F1 LogReg:", f1_score(y_test, y_pred_test_logreg, pos_label = 'republican'))
# F1 LogReg: 0.9803921568627451
